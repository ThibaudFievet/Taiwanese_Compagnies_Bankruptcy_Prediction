{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca018aa",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: Arial\">\n",
    "    <h1 style=\"color: #2c3e50\">Final Project</h1>\n",
    "    <h2 style=\"color: #34495e\">7957 - DATA MINING & MACHINE LEARNING</h2>\n",
    "    <h3 style=\"color: #7f8c8d\">Thibaud Fievet</h3>\n",
    "    <hr style=\"width: 50%; border: 1px solid #bdc3c7;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cef709",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55224a43",
   "metadata": {},
   "source": [
    "The selected Taiwanese Bankruptcy Predictions dataset is suitable for this project because:\n",
    "\n",
    "1 - First, it fulfills the structural, size requirements:\n",
    "\n",
    "a) Instances: The dataset contains 6,819 instances > 5,000\n",
    "\n",
    "b) Features: It includes 95 features > 15\n",
    "\n",
    "2 - Most importantly, it is ideal for two out of 3 machine learning tasks:\n",
    "\n",
    "a) For Classification: The dataset's primary value lies in its explicit binary target variable ('y'), which indicates whether a company went bankrupt.\n",
    "\n",
    "b) For Clustering: The high dimensionality provides a rich dataset for unsupervised learning. By applying clustering algorithms  to these features, we can explore whether distinct, naturally occurring \"profiles\" of companies (for example 'financially stable', 'at-risk' or 'high-growth/high-debt') exist within the data, independent of the final bankruptcy label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e287f7",
   "metadata": {},
   "source": [
    "First let's import libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75788015",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data.csv')\n",
    "    print(\"Dataset loaded succesfuly. Number of instances  :\", len(df))\n",
    "except FileNotFoundError:\n",
    "    print(\"Error : File 'data.csv' not find.\")\n",
    "    df = pd.DataFrame() \n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8cdc5f",
   "metadata": {},
   "source": [
    "There's a little space at the beginning of each title of features, we will remove them in order to avoid any future mistake.\n",
    "\n",
    "We also noticed that there's 1 constant columnso we will get rid of her. \n",
    "\n",
    "There's also another column that has only 8 out of 6819 \"1\" value (0.11%). We can clearly consider that this is a near-zero variance predictor \n",
    "so will remove her too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns before cleanning :\")\n",
    "print(df.columns[:5])\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"Columns after cleanning :\")\n",
    "print(df.columns[:5])\n",
    "\n",
    "unique_counts = df.nunique()\n",
    "\n",
    "constant_cols = unique_counts[(unique_counts <= 2) & (unique_counts.index != 'Bankrupt?')].index.tolist() #only keep column where there's only 1 or 2 unique value except the target. Then get their names and convert them to a simple python list\n",
    "\n",
    "if len(constant_cols) > 0:\n",
    "    print(f\"Found {len(constant_cols)} constant column(s) (zero or near zero variance):\")\n",
    "    print(constant_cols)\n",
    "    \n",
    "    df = df.drop(columns=constant_cols)\n",
    "    \n",
    "    print(f\"New DataFrame shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"No constant columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f526003",
   "metadata": {},
   "source": [
    "In this part we will define target and features.\n",
    "\n",
    "We will also find the top 15 features positively and top15 negatively with Bankruptcy for the visuals of the following analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = 'Bankrupt?'\n",
    "\n",
    "FEATURE_COLS = df.columns.drop(TARGET_COL)\n",
    "\n",
    "print(f\"Target Variable : '{TARGET_COL}'\")\n",
    "print(f\"Features to analyse : {len(FEATURE_COLS)}\")\n",
    "\n",
    "# Correlation calcul for all columns with TARGET_COL (Bankruptcy)\n",
    "correlations_target = df.corr()[TARGET_COL]\n",
    "\n",
    "# Ensure that we remove colum of the target, remove any other potential NaNs, and sort\n",
    "correlations_features = correlations_target.drop(TARGET_COL).dropna().sort_values()\n",
    "\n",
    "top_15_neg_features = correlations_features.head(15)\n",
    "\n",
    "top_15_pos_features = correlations_features.tail(15)\n",
    "\n",
    "top_30_features = pd.concat([top_15_neg_features, top_15_pos_features])\n",
    "\n",
    "print(f\"Top 15 'protective' features:\\n{top_15_neg_features}\\n\")\n",
    "print(f\"Top 15 'risky' features:\\n{top_15_pos_features}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4843e",
   "metadata": {},
   "source": [
    "Now we will perform Univariate analysis. \n",
    "\n",
    "Since we found the top30 correlated variables to Bankruptcy, we're going to show some visuals for both of them. I choose to not display visuals for all of the features as it will overload the output and won't be very relevant. Some comments about how i proceed: I used the transpose at the beginning to make the output more readable (features as rows and stats as columns). I also transformed the grid into a list of 30 locations in which i enumerate, for each feature, the name of the feature in the right position. Here tight_layout is very important in order to avoid overlapping titles, and improve presentation.\n",
    "\n",
    "The visuals for all features are in comments and in it, there will be 2 locations where there's nothing: it correspond to the 2 features that we dropped earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[FEATURE_COLS].describe().T)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(18, 30)) \n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(top_30_features.index):\n",
    "    if col in df.columns:\n",
    "        df[col].hist(bins=20, ax=axes[i]) #axes0 at i=0\n",
    "        axes[i].set_title(col, fontsize=10)\n",
    "    else:\n",
    "        axes[i].set_title(f\"'{col}' not find\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of top30 features the most correlated \", y=1.01, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\"\"\" \n",
    "fig_all, axes_all = plt.subplots(nrows=19, ncols=5, figsize=(30, 60))\n",
    "\n",
    "axes_all = axes_all.flatten()\n",
    "\n",
    "for i, col in enumerate(FEATURE_COLS):\n",
    "    if col in df.columns:\n",
    "        df[col].hist(bins=30, ax=axes_all[i])\n",
    "        axes_all[i].set_title(col, fontsize=10) \n",
    "    else:\n",
    "        axes_all[i].set_title(f\"'{col}' not find\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Distribution of all features\", y=1.01, fontsize=20)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2c366",
   "metadata": {},
   "source": [
    "For the Bivariate Analysis, we will compare the distribution of these 30 features with compagnies which bankrupt and those who didn't. \n",
    "\n",
    "Same idea as before, only 30 otherwise it will be overload. Some visuals were unreadable as the values were really close to each others, this is why I put showfliers at False in order to zoom on these unreadable visuals. \n",
    "\n",
    "I also implemented a heatmap in order to find which features calculate the same things and are redundants (multi-colinearity problem). Annot parameter is at False as we don't want the values to be written in each box because it is not readable. Vmin and vmax are between -1 and 1 as a correlation is located in this range. \n",
    "\n",
    "Thus, we will mostly follow the same steps in the code as in the univariate analysis except that the visuals are differents. Note that in the first loop, we removed x-axis label (target labels) and y-axis label (feature labels). Thus, 0 will mean no bankrupt and 1 will mean bankrupt but we still have the title of the feature at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10580241",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_30_feature_list = top_30_features.index\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(18, 30)) \n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(top_30_feature_list):\n",
    "    \n",
    "    current_ax = axes[i] # Get the current axis to plot on. Before, we implemented it directly in the df[col].hist\n",
    "    \n",
    "    if col in df.columns:\n",
    "        sns.boxplot(x=TARGET_COL, y=col, data=df, ax=current_ax, showfliers=False)\n",
    "        current_ax.set_title(col, fontsize=10)\n",
    "        current_ax.set_xlabel('')\n",
    "        current_ax.set_ylabel('') \n",
    "    else:\n",
    "        current_ax.set_title(f\"'{col}' not found\", fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Bivariate Analysis: Top 30 Features vs Bankruptcy Status\", y=1.01, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "corr_matrix = df[FEATURE_COLS].corr() # Pearson matrix\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation matrix of all features\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d136c26",
   "metadata": {},
   "source": [
    "# Machine Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c40020",
   "metadata": {},
   "source": [
    "For this project, we will apply Classification and Clustering.\n",
    "\n",
    "These choices are directly motivated by the structure of the dataset and the analytical goals. In the following lines, we may repeat what we said in the introduction of the descriptive analysis.\n",
    "\n",
    "Task 1: Classification\n",
    "This type of analysis is interesting because the dataset contains a clear, binary target variable (Bankrupt? = Yes or No). This makes it perfectly suited for a classification task. The objective is to build a predictive model that can accurately identify which companies are at high risk of bankruptcy based on their financial ratios.\n",
    "I choose this method because we must choose classification as our goal is prediction and our target variable is categorical (0 or 1).\n",
    "\n",
    "Task 2: Clustering\n",
    "This type of analysis is interesting because the dataset features 95 financial ratios, providing a rich, high-dimensional space for unsupervised learning. The goal of clustering  is discovery. We aim to identify if naturally distinct \"profiles\" or \"archetypes\" of companies (e.g., 'stable low-risk', 'high-growth/high-debt', 'at-risk') exist within the data, based only on their financial features, not their bankruptcy status.\n",
    "I choose this method because this complements classification. While classification predicts an outcome, clustering describes the hidden structure of the data, that allows us to define patterns/clusters.\n",
    "\n",
    "Why not Regression ?\n",
    "The choice to use Classification was mandatory and the most coherent. Our target variable (Bankrupt?) is binary, not a continuous numerical value. \n",
    "Indeed, the choice to exclude Regression was also mandatory. Regression models are designed to predict continuous values (e.g., a stock price or a housing price). Attempting to use regression for a binary outcome is inappropriate for this problem.\n",
    "Therefore, Clustering was selected as the logical second method. It offers a completely different perspective on the data, fulfilling the project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017efeb",
   "metadata": {},
   "source": [
    "Let's start by defining our train and test dataset. \n",
    "\n",
    "We can do so by splitting our dataset in two, but not naively. We're going to see that we only have 220 compagnies that \n",
    "actually bankrupt over 6819. It is widely disproportionate and we need to adress this issue in our train model otherwise our model will be wrong. Thus, one possiblity is to fill it with only 220 compagnies that didn't bankrupt and all which did bankrupt. In this case, we will have a perfectly balanced train dataset but that would be problematic for the test of the model. \n",
    "\n",
    "Indeed, we will test our model in a fully \"clean\" dataset of compagnies (that didn't bankrupted). In order to adress this other issue, we will split our dataset in 80% for train and 20% for test and we will use stratify=y in order to ensure that both train and test sets have the same 3.2% ratio of bankruptcies (maximum ratio with respect to our dataset) after the split. \n",
    "\n",
    "I also computed the first solution as comments. Note that we used random_state to ensure that we always have the same samples every time (=42 as a convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da079a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=TARGET_COL)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Total instances: {len(X)}\")\n",
    "print(f\"Training instances: {len(X_train)}\")\n",
    "print(f\"Test instances: {len(X_test)}\")\n",
    "\n",
    "print(\"\\n--- Training set target distribution ---\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "print(f\"Nombre of bankrup in y_test : {sum(y_train == 1)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Test set target distribution ---\")\n",
    "print(y_test.value_counts(normalize=True) * 100)\n",
    "print(f\"Number of bankrupt in y_test: {sum(y_test == 1)}\")\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_bankrupt = df[df[TARGET_COL] == 1] # 220 instances\n",
    "df_solvent = df[df[TARGET_COL] == 0]  # 6599 instances\n",
    "\n",
    "print(f\"Bankrupt instances: {len(df_bankrupt)}\")\n",
    "print(f\"Solvent instances: {len(df_solvent)}\")\n",
    "\n",
    "df_solvent_sample = df_solvent.sample(n=220, random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_bankrupt, df_solvent_sample])\n",
    "\n",
    "df_train = shuffle(df_train, random_state=42) #shuffle so the training set doesn't see all 1's and then all 0's as bankrupt value. This would lead to an error.\n",
    "\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "print(f\"\\nTraining set shape: {df_train.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "\n",
    "y_train = df_train[TARGET_COL]\n",
    "X_train = df_train.drop(columns=TARGET_COL)\n",
    "\n",
    "y_test = df_test[TARGET_COL]\n",
    "X_test = df_test.drop(columns=TARGET_COL)\n",
    "\n",
    "print(f\"\\nTraining data (y_train) distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTest data (y_test) distribution:\\n{y_test.value_counts()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273321ba",
   "metadata": {},
   "source": [
    "We need to standardise our datas because they don't have the same scale and this will be an issue for the differents techniques we're going to use in order to train our model. \n",
    "\n",
    "We will make the scaler analyse our data (find the mean and the standard deviation) and transform each values of X_train (minus the mean and divide by the standard deviation). We do this in order to have all values at the same scale (= to avoid imbalance of features). \n",
    "\n",
    "For the scaler appplication on test datas, we don't use .fit (= we don't make our scaler analyse these datas) as they are the test datas.\n",
    "\n",
    "We know that we have a lot of features and, based on the previous heatmap, some features may appears redondant (multicollinearity).\n",
    "\n",
    "To address this, we performed a supplementary step using Lasso Regression.\n",
    "\n",
    "The algorithm successfully eliminated 35 features (setting their coefficients to exactly zero), retaining only 58 critical variables out of the original 94. This confirms our hypothesis that the dataset contained significant redundancy and multicollinearity (e.g., multiple variations of the same financial ratio).\n",
    "\n",
    "We also found useful to know which features were the strongest predictor so we computed an output to showcase this.\n",
    "\n",
    "Persistent EPS in the Last Four Seasons (Coeff: -0.98) is the strongest predictor. This indicates that consistent, long-term earnings per share is the best shield against bankruptcy.\n",
    "Equity to Liability (Coeff: -0.95) and Net Income to Total Assets (Coeff: -0.93) follow closely. This perfectly aligns with financial theory: companies with low leverage (high equity vs. liability) and high asset profitability are structurally safe.\n",
    "\n",
    "In the other hand, Total debt/Total net worth (Coeff: +0.59) and Borrowing dependency (Coeff: +0.54) appear as top positive coefficients. As expected, these are the primary warning signs: higher reliance on debt directly increases the probability of bankruptcy.\n",
    "\n",
    "The Lasso analysis reveals that solvency is primarily predicted by a company's ability to generate consistent earnings (EPS) and maintain a strong equity cushion, rather than just short-term cash flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Shape of scaled X_train: {X_train_scaled.shape}\")\n",
    "print(f\"Shape of scaled X_train: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\n--- Training of Lasso for selection ---\")\n",
    "lasso_selector = LogisticRegression(penalty='l1', solver='liblinear', C=0.5, random_state=42, max_iter=1000)\n",
    "lasso_selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "coefs = lasso_selector.coef_.flatten()\n",
    "\n",
    "selection_mask = coefs != 0\n",
    "\n",
    "selected_features_names = X_train.columns[selection_mask]\n",
    "print(f\"Lasso kept {sum(selection_mask)} features on {len(selection_mask)}.\")\n",
    "print(f\"Deleted Features : {len(selection_mask) - sum(selection_mask)}\")\n",
    "\n",
    "print(\"\\n--- Dataset reduction ---\")\n",
    "X_train_reduced = X_train_scaled[:, selection_mask]\n",
    "X_test_reduced = X_test_scaled[:, selection_mask]\n",
    "\n",
    "print(f\"New shape Train : {X_train_reduced.shape}\")\n",
    "print(f\"New shape Test  : {X_test_reduced.shape}\")\n",
    "\n",
    "lasso_results = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefs, 'Abs_Coefficient': np.abs(coefs)})\n",
    "\n",
    "print(\"\\n--- Top 10 Most Critical Features (Selected by Lasso) ---\")\n",
    "print(lasso_results.sort_values(by='Abs_Coefficient', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88574808",
   "metadata": {},
   "source": [
    "Now let's train our model on the logistic regression and show some visuals of it's performance. \n",
    "\n",
    "We increase max_iter to 1000 in order to avoid ConvergenceWarning as our dataset has still a lot of features. We will make predictions on the test set.\n",
    "\n",
    "We used target_names in order to make the report clearer (instead of 0 and 1) in the classification_report parameters. In ConfusionMatrixDisplay and RocCurveDisplay, we don't need y_pred_log as parameters as these metrics commands will do it automaticaly. We also used RocAucCurve for visuals as the plot displayed is excellent for showing model performance on imbalanced data. A score of 0.5 is random guessing, 1.0 is perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "log_reg.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_pred_log = log_reg.predict(X_test_reduced)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_log, target_names=['Solvent (0)', 'Bankrupt (1)'])) \n",
    "\n",
    "print(\"\\nConfusion Matrix\")\n",
    "ConfusionMatrixDisplay.from_estimator(log_reg, X_test_reduced, y_test, display_labels=['Solvent', 'Bankrupt'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nROC-AUC Curve\")\n",
    "RocCurveDisplay.from_estimator(log_reg, X_test_reduced, y_test, name='Logistic Regression')\n",
    "plt.title(\"ROC-AUC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb98a8",
   "metadata": {},
   "source": [
    "To robustly evaluate the stability of our model after Lasso feature selection, we performed a 5-fold Cross-Validation using the reduced dataset (X_train_reduced).\n",
    "\n",
    "The cross_val_score function splits the training data into 5 folds. It automatically applies stratification, ensuring that the minority class (bankrupt companies) is represented proportionally in every fold without needing manual specification.\n",
    "We utilized scoring='roc_auc' as our evaluation metric. Given the dataset's severe imbalance, standard accuracy would be misleading. ROC-AUC provides a trustworthy measure of the model's discriminative ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e71856",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(log_reg, X_train_reduced,y_train, cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-Validation ROC-AUC Scores (5 Folds): {cv_scores}\")\n",
    "print(f\"Mean CV ROC-AUC Score: {cv_scores.mean():.4f}\")\n",
    "print(f\"Std Dev CV ROC-AUC Score: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb5f68",
   "metadata": {},
   "source": [
    "The Logistic Regression model demonstrated strong stability and predictive potential, evidenced by a mean Cross-Validation ROC-AUC of 0.9102 and a consistent Test ROC-AUC of 0.91. While the overall Accuracy appears high at 96%, this metric is heavily skewed by the majority 'Solvent' class. A deeper look at the critical 'Bankrupt' class reveals that the model is operationally conservative at the default decision threshold: it achieved a Precision of 0.38 (one in three bankruptcy alerts was correct) and a low Recall of 0.18 (identifying only 18% of actual bankruptcies), resulting in a F1-Score of 0.25. This contrast between the excellent AUC (0.91) and the low Recall (0.18) proves that the model effectively ranks risky companies higher than healthy ones, but the standard 50% probability threshold is too strict to capture the majority of bankruptcy cases in a real-world setting.\n",
    "\n",
    "Also, as we say before, while the Weighted Average F1-score of 0.96 might suggest near-perfect performance, it is heavily biased by the majority class. The Macro Average F1-score of 0.61 offers a more honest assessment, reflecting the disparity between the model's excellent detection of healthy companies and its struggle to recall bankruptcies. Furthermore, the low Cross-Validation Standard Deviation (0.0263) is a critical indicator of success, proving that despite the recall challenges, the model's stability and ranking ability (AUC) are robust and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bab7de",
   "metadata": {},
   "source": [
    "To fulfill the project's requirement of applying a second machine learning technique, we will now perform Clustering.\n",
    "\n",
    "We will use the K-Means algorithm to analyze all 94 financial features to see if the data naturally separates into distinct 'profiles' or groups of companies based on their financial characteristics, independent of the bankruptcy label. It is worth noting that the feature selection performed by Lasso could also be beneficial here: running K-Means on the reduced set of critical features would likely minimize noise and produce even more distinct financial profiles. However, We decided to perform the clustering analysis on the full set of 94 features. Since Lasso is a supervised technique that filters variables based on their direct relationship with the target (Bankrupt?), using it here would have biased our exploratory analysis solely towards risk factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f54b3",
   "metadata": {},
   "source": [
    "The first step in K-Means clustering is to determine the optimal number of clusters (K). We used the Elbow Method for this analysis.\n",
    "\n",
    "Because K-Means is a distance-based algorithm, we first scaled the entire dataset (unlike in the logistic regression) as, for clustering, there's no 'test' set, using StandardScaler to ensure all features are weighted equally. We then ran the K-Means algorithm for K-values 2 through 10 (only to 10 because more might not be relevant and it is very unlikely that we have more than 10 clusters). For each run, we recorded the model's inertia metric (using property inertia_) measuring how compact/tight the clusters are. By plotting this inertia against the K-values, we can look for the \"elbow\" point, which represents the optimal K where adding more clusters no longer provides a significant improvement. \n",
    "\n",
    "Also, we set n_init=10 to avoid \"false start\" because of a misleading first random point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ae056",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled_full = scaler.fit_transform(X)\n",
    "\n",
    "inertia_scores = []\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled_full)\n",
    "    \n",
    "    inertia_scores.append(kmeans.inertia_)\n",
    "    print(f\"Completed K={k}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, inertia_scores, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1837e5",
   "metadata": {},
   "source": [
    "Based on previous results, we can clearly consider that an optimal elbow is 6 as the curve seems to flattens above this point especially from 6 to 7 clusters. \n",
    "\n",
    "We will thus use the value of cluster for our model. We used labels_ in order to asign each compagny a cluster, we will then have an array where each items is the cluster number (0-5) for the corresponding compagny. After this, we created a copy of our original X dataset (without bankruptcy, not scaled), and add to the copy a column 'cluster'. This step is essential for the subsequent analysis, allowing us to profile the companies within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb6ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_K = 6\n",
    "\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\n",
    "kmeans.fit(X_scaled_full)\n",
    "\n",
    "cluster_labels = kmeans.labels_ \n",
    "\n",
    "X_with_clusters = X.copy()\n",
    "X_with_clusters['Cluster'] = cluster_labels\n",
    "\n",
    "print(X_with_clusters.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b8ca9",
   "metadata": {},
   "source": [
    "Let's dive to the cluster analysis, the most important part of clustering. We will find the \"average profile\" for each of the 6 clusters.\n",
    "\n",
    "We will group the data by cluster and calculate the mean of some of the most important financial ratios (based on our Top 30 we used for univariate and bivariate analysis) by using groupby('Cluster').mean (Cluster which is the new column we add before). We will have a new table with rows which are our 6 clusters (0-5), and the columns are all 94 features. Each cell will then shows the average value of that specific feature for that specific cluster. I decided to only display half of the most negatives correlated features and half of the most positive correlated features for each cluster as we don't want to have unreadable output and not useful information\n",
    "\n",
    "We used .union() because we are combining Index objects (the lists of column names), not DataFrames, using pd.concat() as we did before would be incorrect here, as it's designed to stick entire DataFrames together, not to combine lists of names.\n",
    "\n",
    "As a final analysis, we re-introduced the original Bankrupt? label to set X_with_clusters in order to calculate the average bankruptcy rate within each cluster, allowing us to see if our unsupervised model successfully isolated the high-risk companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24255276",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_7_neg = top_15_neg_features.head(7).index\n",
    "\n",
    "top_7_pos = top_15_pos_features.tail(7).index\n",
    "\n",
    "features_to_analyze_14 = top_7_neg.union(top_7_pos)\n",
    "\n",
    "cluster_profiles = X_with_clusters.groupby('Cluster').mean()\n",
    "\n",
    "print(cluster_profiles[features_to_analyze_14].T)\n",
    "\n",
    "if 'Bankrupt' not in X_with_clusters.columns:\n",
    "    X_with_clusters['Bankrupt'] = y\n",
    "\n",
    "cluster_bankruptcy_rates = X_with_clusters.groupby('Cluster')['Bankrupt'].mean() * 100\n",
    "\n",
    "print(\"\\n--- Bankruptcy Rate (%) per Cluster ---\")\n",
    "print(cluster_bankruptcy_rates.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27409e65",
   "metadata": {},
   "source": [
    "The analysis of the cluster means shows clear \"personalities.\" For example, Clusters 0 and 2 appear to be the 'Healthy' profiles, characterized by very low Debt ratio % (0.07% and 0.05%) and the highest Net worth/Assets (0.92 and 0.95). In contrast, Cluster 3 is a clear outlier, showing significantly higher debt (Debt ratio %: 0.29) and lower profitability (ROA(A): 0.35) than the others groups.\n",
    "\n",
    "The most critical finding comes from the bankruptcy rate analysis. This confirms our profiling:\n",
    "\n",
    "Cluster 3 (The 'High-Risk' Profile): This group, which the model identified based on for example its high-debt profile, has a 75% bankruptcy rate.\n",
    "\n",
    "Cluster 5 (The 'At-Risk' Profile): This group shows a moderately high 15.44% bankruptcy rate.\n",
    "\n",
    "Clusters 0 & 2 (The 'Healthy' Profiles): These groups had a 0% bankruptcy rate. We can clearly also include clusters 4 and 1 in this group\n",
    "\n",
    "This result is a major success. It proves that the unsupervised K-Means model was able to successfully isolate the high-risk companies from the healthy ones using only their financial data, and without ever seeing the Bankrupt? label during its training.\n",
    "\n",
    "In hindsight, after seeing these results, we can see that a simpler model with K=3 (which was also a visible 'elbow') might have been a viable alternative. It would likely have captured the three main risk profiles: a \"High-Risk\" group (our Cluster 3), an \"At-Risk\" group (our Cluster 5), and one large \"Healthy\" group (combining our other clusters). However, our choice of K=6 provided a more granular and detailed view of the healthy population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fbe77",
   "metadata": {},
   "source": [
    "In conclusion, these results are highly conclusive. Both models proved to be stable and insightful, generating actionable value from complex financial data. The clustering successfully isolated risk profiles, while the classification demonstrated strong predictive stability despite the challenges of the severe class imbalance.\n",
    "\n",
    "## Final Conclusion and Acknowledgements\n",
    "Thank you for taking the time to read this report. I thoroughly enjoyed working on this project and found it to be an incredibly enriching experience. The entire process, from data cleaning to interpretation, has strongly motivated me to continue pursuing this type of analysis in the future.\n",
    "\n",
    "While both models were insightful, I found the clustering task to be particularly fascinating. I believe the outputs from unsupervised learning, which reveal hidden structures and natural profiles within the data, are exceptionally valuable.\n",
    "\n",
    "This project was completed as part of my Data mining & Machine Learning course at EDHEC Business School. I would like to extend my gratitude to our professor, Mr. Christophe Desagre, who not only taught us these models but also provided guidance and support throughout this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
